{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced classification.\n",
    "\n",
    "PyOD XGBOD\n",
    "\n",
    "Binary Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/supervised-machine-learning-technique-for-anomaly-detection-logistic-regression-97fc7a9cacd4\n",
    "\n",
    "- https://stats.stackexchange.com/questions/474640/is-anomaly-detection-supervised-or-un-supervised\n",
    "\n",
    "- https://github.com/yzhao062/pyod\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2021/06/univariate-anomaly-detection-a-walkthrough-in-python/\n",
    "\n",
    "- https://www.projectpro.io/article/anomaly-detection-using-machine-learning-in-python-with-example/555\n",
    "\n",
    "- https://medium.com/learningdatascience/anomaly-detection-techniques-in-python-50f650c75aaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this section because of i will focus on models i will not implement feature selection, feature elimination or dimensionality reduction. But feature selection and hyperparameter optimization are must while preparing a machine learning model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an imbalanced data(like 90000 labeled with 1 and 100 data labeled 0) you can look the links below.\n",
    "\n",
    "- https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n",
    "\n",
    "- https://www.kaggle.com/klaudiajankowska/binary-classification-multiple-method-comparison\n",
    "\n",
    "- https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets\n",
    "\n",
    "- https://www.kaggle.com/satoshiss/titanic-binary-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"feature_engineering_tools/df_train_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.113551</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.009889</td>\n",
       "      <td>-0.039310</td>\n",
       "      <td>-0.091223</td>\n",
       "      <td>-0.006301</td>\n",
       "      <td>-0.091933</td>\n",
       "      <td>-0.02622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.813985</td>\n",
       "      <td>-0.779157</td>\n",
       "      <td>-0.280673</td>\n",
       "      <td>0.073120</td>\n",
       "      <td>-0.287993</td>\n",
       "      <td>-0.641804</td>\n",
       "      <td>-0.627365</td>\n",
       "      <td>-0.221668</td>\n",
       "      <td>-0.374281</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.113551</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.010032</td>\n",
       "      <td>-0.039310</td>\n",
       "      <td>-0.091223</td>\n",
       "      <td>-0.006301</td>\n",
       "      <td>-0.091933</td>\n",
       "      <td>-0.02622</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030895</td>\n",
       "      <td>-1.157831</td>\n",
       "      <td>2.764403</td>\n",
       "      <td>2.375620</td>\n",
       "      <td>-0.287993</td>\n",
       "      <td>-0.641804</td>\n",
       "      <td>-0.627365</td>\n",
       "      <td>-0.385140</td>\n",
       "      <td>-0.374281</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.113551</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.010093</td>\n",
       "      <td>-0.039310</td>\n",
       "      <td>-0.091223</td>\n",
       "      <td>-0.006301</td>\n",
       "      <td>-0.091933</td>\n",
       "      <td>-0.02622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.804947</td>\n",
       "      <td>-0.935081</td>\n",
       "      <td>-0.173828</td>\n",
       "      <td>-0.478183</td>\n",
       "      <td>-0.287993</td>\n",
       "      <td>1.603834</td>\n",
       "      <td>1.614454</td>\n",
       "      <td>-0.385140</td>\n",
       "      <td>-0.374281</td>\n",
       "      <td>anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.113551</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.009996</td>\n",
       "      <td>0.052473</td>\n",
       "      <td>-0.091223</td>\n",
       "      <td>-0.006301</td>\n",
       "      <td>-0.091933</td>\n",
       "      <td>-0.02622</td>\n",
       "      <td>...</td>\n",
       "      <td>1.264742</td>\n",
       "      <td>1.069663</td>\n",
       "      <td>-0.440940</td>\n",
       "      <td>-0.380894</td>\n",
       "      <td>0.073759</td>\n",
       "      <td>-0.574435</td>\n",
       "      <td>-0.604947</td>\n",
       "      <td>-0.385140</td>\n",
       "      <td>-0.342768</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.113551</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.010010</td>\n",
       "      <td>-0.034582</td>\n",
       "      <td>-0.091223</td>\n",
       "      <td>-0.006301</td>\n",
       "      <td>-0.091933</td>\n",
       "      <td>-0.02622</td>\n",
       "      <td>...</td>\n",
       "      <td>1.264742</td>\n",
       "      <td>1.069663</td>\n",
       "      <td>-0.440940</td>\n",
       "      <td>-0.478183</td>\n",
       "      <td>-0.287993</td>\n",
       "      <td>-0.641804</td>\n",
       "      <td>-0.627365</td>\n",
       "      <td>-0.385140</td>\n",
       "      <td>-0.374281</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration  protocol_type  service  flag  src_bytes  dst_bytes  \\\n",
       "0 -0.113551              2       51     7  -0.009889  -0.039310   \n",
       "1 -0.113551              3       33     7  -0.010032  -0.039310   \n",
       "2 -0.113551              2        8     2  -0.010093  -0.039310   \n",
       "3 -0.113551              2       61     7  -0.009996   0.052473   \n",
       "4 -0.113551              2       61     7  -0.010010  -0.034582   \n",
       "\n",
       "   wrong_fragment    urgent       hot  num_failed_logins  ...  \\\n",
       "0       -0.091223 -0.006301 -0.091933           -0.02622  ...   \n",
       "1       -0.091223 -0.006301 -0.091933           -0.02622  ...   \n",
       "2       -0.091223 -0.006301 -0.091933           -0.02622  ...   \n",
       "3       -0.091223 -0.006301 -0.091933           -0.02622  ...   \n",
       "4       -0.091223 -0.006301 -0.091933           -0.02622  ...   \n",
       "\n",
       "   dst_host_srv_count  dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0           -0.813985               -0.779157               -0.280673   \n",
       "1           -1.030895               -1.157831                2.764403   \n",
       "2           -0.804947               -0.935081               -0.173828   \n",
       "3            1.264742                1.069663               -0.440940   \n",
       "4            1.264742                1.069663               -0.440940   \n",
       "\n",
       "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                     0.073120                    -0.287993   \n",
       "1                     2.375620                    -0.287993   \n",
       "2                    -0.478183                    -0.287993   \n",
       "3                    -0.380894                     0.073759   \n",
       "4                    -0.478183                    -0.287993   \n",
       "\n",
       "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0             -0.641804                 -0.627365             -0.221668   \n",
       "1             -0.641804                 -0.627365             -0.385140   \n",
       "2              1.603834                  1.614454             -0.385140   \n",
       "3             -0.574435                 -0.604947             -0.385140   \n",
       "4             -0.641804                 -0.627365             -0.385140   \n",
       "\n",
       "   dst_host_srv_rerror_rate    class  \n",
       "0                 -0.374281   normal  \n",
       "1                 -0.374281   normal  \n",
       "2                 -0.374281  anomaly  \n",
       "3                 -0.342768   normal  \n",
       "4                 -0.374281   normal  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i dont have to tackle with unbalance data because the data is not unbalanced but i will act like data is like that.\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms will use:\n",
    "\n",
    "+Logistic Regression(If you are working on a textual dataset where the data is not very large then it is good)\n",
    "\n",
    "-Stochastic Gradient Descent Classifier(If you are working on a large dataset of images then you have to use a very powerful classification algorithm.)\n",
    "\n",
    "+Passive-Agressive(If you are working on a binary classification problem where the data arrives in a continuous flow)\n",
    "\n",
    "+lightGBM\n",
    "\n",
    "+extra trees\n",
    "\n",
    "-regularized greedy forest\n",
    "\n",
    "-neural networks\n",
    "\n",
    "+k-NN\n",
    "\n",
    "-naive bayes\n",
    "\n",
    "-mahalanobis binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPARING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About Data**\n",
    "\n",
    "39 columns, binary class(anomaly and normal), 20k rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, precision_score, \\\n",
    "roc_curve, roc_auc_score, mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide train, test and validation sets equally. All of them have equal rate of class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2, df_holdout = train_test_split(df_train, test_size=0.1, stratify=df_train[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test, df_val = train_test_split(df_holdout, test_size=0.5, stratify=df_holdout[\"class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compare models save the scores of each model to dictionary\n",
    "compare_dictionary = {}\n",
    "\n",
    "def model_evaluation(model, params, plot_cm=False, show_best_params=False, show_metrics=False):\n",
    "    \"\"\"\n",
    "    A quick model evaluation. The time metric used here only for comparing the models' speed.\n",
    "    \"\"\"\n",
    "    sample_time = time.time()\n",
    "    \n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    x_train, y_train = df_train2.drop(\"class\", axis=1), df_train2[\"class\"]\n",
    "    cv_score = np.mean(cross_val_score(model, x_train, y_train, cv=5))\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    # I use df_test set to detect models hyperparameters. And i will make final evaluation on df_val.\n",
    "    \n",
    "    x_test, y_test = df_test.drop(\"class\", axis=1), df_test[\"class\"]\n",
    "    random_cv = RandomizedSearchCV(model, params, n_iter=10, cv=5)\n",
    "    random_cv.fit(x_test, y_test)\n",
    "    \n",
    "    best_estimator = random_cv.best_estimator_\n",
    "    best_score_test = round(random_cv.best_score_, 3)\n",
    "    best_params = random_cv.best_params_\n",
    "    \n",
    "    x_val, y_val = df_val.drop(\"class\", axis=1), df_val[\"class\"]\n",
    "    best_score_val = np.round(best_estimator.score(x_val, y_val), 4)\n",
    "    \n",
    "    print(f\"FOR MODEL {model_name}\")\n",
    "    print(f\"Pure Test Score = {best_score_test}\")\n",
    "    print(f\"Final score = {best_score_val}\")\n",
    "    \n",
    "    if show_best_params:\n",
    "        print(f\"Best Parameters = {best_params}\")\n",
    "        \n",
    "    if show_metrics:\n",
    "        #precision recall f1 score, roc auc curve\n",
    "        \n",
    "        y_pred = model.predict(x_val)\n",
    "        y_true = y_val.values\n",
    "\n",
    "        y_pred = [1 if i==\"normal\" else 0 for i in y_pred]\n",
    "        y_true = [1 if i==\"normal\" else 0 for i in y_true]\n",
    "        \n",
    "        print(f\"Precision Score = {np.round(precision_score(y_pred, y_true),3)}\\\n",
    "\\nRecall Score = {np.round(recall_score(y_pred, y_true),3)}\\nF1 Score = {np.round(f1_score(y_pred, y_true),3)}\")\n",
    "        rocauc_score = np.round(roc_auc_score(y_true, y_pred),3)\n",
    "        # How negative and positive classes divided good. If 1 best distinction, otherwise 0 worst distinction.\n",
    "        print(f\"Roc Auc Score = {rocauc_score}\")\n",
    "        \n",
    "        fpr1, tpr1, thresh1 = roc_curve(y_true, y_pred)\n",
    "\n",
    "        plt.plot(fpr1, tpr1, linestyle=\"--\", color=\"black\")\n",
    "        plt.title(f\"ROC Curve for {model_name}\")\n",
    "        plt.show()\n",
    "        \n",
    "    if plot_cm:\n",
    "        y_pred = model.predict(x_val)\n",
    "        y_true = y_val\n",
    "        \n",
    "        cm = confusion_matrix(y_pred, y_true)\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt=\".3g\")\n",
    "        plt.title(f\"Confusion Matrix for {model_name}\")\n",
    "        plt.show()\n",
    "    \n",
    "    time_spent = np.round((time.time() - sample_time),3)\n",
    "    print(f\"Time Spent = {time_spent}\")\n",
    "    compare_dictionary.update({model_name:{\"final_score\":best_score_val, \"roc_auc_score\":rocauc_score,\n",
    "                                          \"pure_score_wo_param\":best_score_test,\"time\":time_spent}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.logspace(-4, 4, 50)\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "params = {\"C\":C,\"penalty\":penalty}\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "model_evaluation(log_reg, params, plot_cm=True, show_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIGHTGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_reg = lgb.LGBMClassifier()\n",
    "\n",
    "params = {\"n_estimators\":[50,250,500,1000],\n",
    "         \"learning_rate\":[0.001,0.01,0.1],\n",
    "         \"max_depth\":[-1,1,5,10]}\n",
    "\n",
    "model_evaluation(lgb_reg, params, plot_cm=True, show_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXTRA TREES\n",
    "\n",
    "It is like random forest. But it selects sub samples without replacement. Extra Trees splits trees randomly. Extra Trees are faster than Random Forests.\n",
    "\n",
    "\n",
    "**Source** \n",
    "\n",
    "- https://quantdare.com/what-is-the-difference-between-extra-trees-and-random-forest/\n",
    "\n",
    "- https://stats.stackexchange.com/questions/175523/difference-between-random-forest-and-extremely-randomized-trees\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/#:~:text=The%20Area%20Under%20the%20Curve,the%20positive%20and%20negative%20classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xclf = ExtraTreesClassifier()\n",
    "params = {\"n_estimators\":[50,250,500,1000],\n",
    "         \"max_depth\":[-1,1,5,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(xclf, params, plot_cm=True, show_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PASSIVE-AGGRESSIVE LEARNING\n",
    "\n",
    "Passive-Aggressive algorithms are generally used for large-scale learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_clf = PassiveAggressiveClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"C\":[0.1,0.5,1],\n",
    "          \"max_iter\":[5,50,100,500,1000],\n",
    "          \"tol\":[0.001,0.01,0.1]}\n",
    "\n",
    "model_evaluation(pa_clf, params, plot_cm=True, show_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can handle with recall score adjusting prediction probability threshold. I say this because this passive aggressive model predict 26 0s as 1s i.e. it says 26 anomalies are normal.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN \n",
    "\n",
    "- https://www.youtube.com/watch?v=UqYde-LULfs&ab_channel=ThalesSehnK%C3%B6rting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting optimal k cluster number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = df_train2.drop(\"class\", axis=1), df_train2[\"class\"]\n",
    "\n",
    "x_test, y_test = df_test.drop(\"class\", axis=1), df_test[\"class\"]\n",
    "\n",
    "y_test = [1 if i==\"normal\" else 0 for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = []\n",
    "score_knn = []\n",
    "for i in range(1,25):\n",
    "    knn_clf = KNeighborsClassifier(i)\n",
    "    knn_clf.fit(x_train, y_train)\n",
    "    y_pred = knn_clf.predict(x_test)\n",
    "    y_pred = [1 if i==\"normal\" else 0 for i in y_pred]\n",
    "    score_knn.append(np.sqrt(mean_squared_error(y_pred, y_test)))\n",
    "    k_values.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 is optimal\n",
    "plt.plot(k_values, score_knn)\n",
    "#plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Cluster Number\")\n",
    "plt.ylabel(\"KNN RMSE Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"p\":[1,2], \"leaf_size\":[1,5,25]}\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(knn_clf, params, plot_cm=True, show_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
